{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import lxml\n",
    "import lxml.etree\n",
    "import time\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import random\n",
    "\n",
    "def scrape_rank_season(args):\n",
    "    season, rank, rank_names, id_anchor = args\n",
    "    data_list = []\n",
    "    \n",
    "    # Since we're in a new process, set up the driver again\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    max_rating = 99999 # arbitrarily high max rating for new season\n",
    "    page_idx = 0 # start from first page for new season\n",
    "    \n",
    "    while True:\n",
    "        print(f\"Season: {season}, Rank: {rank}, Fetching page: {page_idx + 1}, Process ID: {os.getpid()}\")\n",
    "        url = f\"https://sc2pulse.nephest.com/sc2/?season={season}&queue=LOTV_1V1&team-type=ARRANGED&us=true&eu=true&kr=true&cn=true&{rank}=true&page={page_idx}&type=ladder&ratingAnchor={max_rating}&idAnchor={id_anchor}&count=1#ladder-top\"\n",
    "        \n",
    "        driver.get(url)\n",
    "        delay = random.uniform(30, 60)\n",
    "        time.sleep(delay) # adjust based on the tolerance of the website\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        ladder_table_container = soup.find('div', id='ladder-table-container')\n",
    "        ratings_on_page = []\n",
    "        \n",
    "        if ladder_table_container:\n",
    "            tbody = ladder_table_container.find('tbody')\n",
    "            rows = tbody.find_all('tr')\n",
    "            if not rows:\n",
    "                print(f\"No more data found on this page for season {season}, rank {rank}.\")\n",
    "                break\n",
    "                \n",
    "            for row in rows:\n",
    "                rating = row.find('td', class_='rating').text.strip()\n",
    "                race_img = row.find('span', class_='race-percentage-entry').find('img', alt=True)\n",
    "                region_img = row.find('img', class_='table-image-long')\n",
    "                race = race_img['alt'] if race_img else 'Random'\n",
    "                region = region_img['alt'] if region_img else 'Unknown'\n",
    "                \n",
    "                data_list.append({'Region': region.upper(), 'Season': int(season), 'MMR': int(rating), 'Rank': rank_names[rank], 'Race': race.title()})\n",
    "                \n",
    "            if ratings_on_page:\n",
    "                max_rating = min(ratings_on_page)\n",
    "            page_idx += 1\n",
    "    \n",
    "    driver.quit()\n",
    "    return data_list\n",
    "\n",
    "def scrape(seasons, ranks):\n",
    "    rank_names = {\n",
    "        'bro': 'Bronze',\n",
    "        'sil': 'Silver',\n",
    "        'gol': 'Gold',\n",
    "        'pla': 'Platinum',\n",
    "        'dia': 'Diamond',\n",
    "        'mas': 'Masters',\n",
    "        'gra': 'Grandmaster'\n",
    "    }\n",
    "    \n",
    "    id_anchor = 1 # remains static\n",
    "    args_list = [(season, rank, rank_names, id_anchor) for season in seasons for rank in ranks]\n",
    "    \n",
    "    # Set up multiprocessing pool\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap_unordered(scrape_rank_season, args_list), total=len(args_list)))\n",
    "    \n",
    "    # Flatten list of lists\n",
    "    data_list = [item for sublist in results for item in sublist]\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_data\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Now calling scrape_all to do the actual scraping and tracking progress\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mscrape_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# tqdm wrapper for as_completed to track progress\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mas_completed(tasks), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(tasks)):\n\u001b[1;32m---> 24\u001b[0m         all_data\u001b[38;5;241m.\u001b[39mextend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_data\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mC:\\Program Files\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_seasons = np.arange(28, 59)  # seasons 28 through 58\n",
    "season_splits = np.array_split(total_seasons, 4)  # split seasons into 2 parts\n",
    "id_anchors = np.arange(len(season_splits))\n",
    "\n",
    "def scrape_with_progress(seasons, id_anchor):\n",
    "    # Here, you should insert the modified `scrape` function I provided earlier.\n",
    "    # Ensure it's adapted to accept and process `seasons` and `id_anchor` correctly.\n",
    "    # For simplicity, this placeholder assumes it returns a list of data.\n",
    "    return scrape(seasons, id_anchor)\n",
    "\n",
    "def scrape_all():\n",
    "    # Using ProcessPoolExecutor for multiprocessing\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        # Prepare for progress bar tracking\n",
    "        tasks = [executor.submit(scrape_with_progress, seasons, id_anchor) for seasons, id_anchor in zip(season_splits, id_anchors)]\n",
    "        all_data = []\n",
    "        \n",
    "        # tqdm wrapper for as_completed to track progress\n",
    "        for future in tqdm(concurrent.futures.as_completed(tasks), total=len(tasks)):\n",
    "            all_data.extend(future.result())\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Now calling scrape_all to do the actual scraping and tracking progress\n",
    "all_data = scrape_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
