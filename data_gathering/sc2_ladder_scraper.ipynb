{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir pandas numpy beautifulsoup4 selenium webdriver-manager lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import lxml\n",
    "import lxml.etree\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(regions, seasons, id_anchor):\n",
    "    data_list = []\n",
    "\n",
    "    # setup selenium web driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    for region in regions:\n",
    "        for season in seasons:\n",
    "            max_rating = 99999 # arbitrarily high max rating for new season\n",
    "\n",
    "            while True:\n",
    "                attempts = 0 # attempts counter\n",
    "                print(f\"Region: {region.upper()}, Season: {season}, MMR: {max_rating}\")\n",
    "                while attempts < 5:\n",
    "                    url = f\"https://sc2pulse.nephest.com/sc2/?season={season}&queue=LOTV_1V1&team-type=ARRANGED&{region}=true&bro=true&sil=true&gol=true&pla=true&dia=true&mas=true&gra=true&page=0&type=ladder&ratingAnchor={max_rating}&idAnchor={id_anchor}&count=1#ladder-top\"\n",
    "                    \n",
    "                    driver.get(url)\n",
    "                    delay = np.random.uniform(1,3) # adjust based on website tolerance\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                    ladder_table_container = soup.find('div', id='ladder-table-container')\n",
    "                    if ladder_table_container:\n",
    "                        tbody = ladder_table_container.find('tbody')\n",
    "                        rows = tbody.find_all('tr')\n",
    "                        if rows:\n",
    "                            min_rating_on_page = max_rating\n",
    "                            for row in rows:\n",
    "                                player_data = {}\n",
    "                                player_data['Season'] = season\n",
    "                                player_data['Region'] = region.upper()\n",
    "\n",
    "                                mmr = int(row.find('td', class_='rating').text.strip())\n",
    "                                player_data['Rating'] = mmr\n",
    "                                min_rating_on_page = min(min_rating_on_page, mmr) # find minimum MMR from current page\n",
    "\n",
    "                                if mmr >= 4800:\n",
    "                                    rank = \"Grandmaster\"\n",
    "                                elif mmr >= 4250 and mmr < 4800:\n",
    "                                    rank = \"Master\"\n",
    "                                elif mmr >= 3120 and mmr < 4250:\n",
    "                                    rank = \"Diamond\"\n",
    "                                elif mmr >= 2680 and mmr < 3120:\n",
    "                                    rank = \"Platinum\"\n",
    "                                elif mmr >= 2280 and mmr < 2680:\n",
    "                                    rank = \"Gold\"\n",
    "                                elif mmr >= 1720 and mmr < 2280:\n",
    "                                    rank = \"Silver\"\n",
    "                                else:\n",
    "                                    rank = \"Bronze\"\n",
    "                                player_data['Rank'] = rank\n",
    "                                \n",
    "                                race_img = row.find('span', class_='race-percentage-entry').find('img', alt=True)\n",
    "                                player_data['Race'] = race_img['alt'].title() if race_img else 'Random'\n",
    "                                \n",
    "                                data_list.append(player_data)\n",
    "                            \n",
    "                            max_rating = min_rating_on_page - 1 # update max_rating for next page\n",
    "                            break # break out of attempts loop upon successful fetch\n",
    "                        else:\n",
    "                            attempts += 1\n",
    "                    else:\n",
    "                        attempts += 1\n",
    "\n",
    "                if attempts == 5:\n",
    "                    print(f\"After 5 attempts, no data could be retrieved for season {season}, region {region}. Moving on...\")\n",
    "                    break # break out of retry loop after 5 attempts\n",
    "                if max_rating <= 0:\n",
    "                    print(f\"Completed scraping for season {season}, region {region}. Exiting...\")\n",
    "                    break # break out of inner while loop if all players for season and region have been parsed\n",
    "\n",
    "    driver.quit()\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['us', 'eu', 'kr', 'cn']\n",
    "total_seasons = np.arange(28, 59) # seasons 28 through 58\n",
    "\n",
    "num_workers = 2 # change based on threads\n",
    "season_splits = np.array_split(total_seasons, num_workers)\n",
    "id_anchors = np.arange(len(season_splits))\n",
    "\n",
    "# parallelize web scraping\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(scrape, regions, seasons, id_anchor) for seasons, id_anchor in zip(season_splits, id_anchors)]\n",
    "    \n",
    "    all_data = []\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Scraping Progress\"):\n",
    "        all_data.extend(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize data by season, region, rank\n",
    "organized_data = {}\n",
    "\n",
    "for row in all_data:\n",
    "    season = row['Season']\n",
    "    region = row['Region']\n",
    "    rank = row['Rank']\n",
    "    \n",
    "    if season not in organized_data:\n",
    "        organized_data[season] = {}\n",
    "    \n",
    "    if region not in organized_data[season]:\n",
    "        organized_data[season][region] = {}\n",
    "        \n",
    "    if rank not in organized_data[season][region]:\n",
    "        organized_data[season][region][rank] = []\n",
    "    \n",
    "    organized_data[season][region][rank].append(row)\n",
    "\n",
    "for season, regions in organized_data.items():\n",
    "    for region, ranks in regions.items():\n",
    "        # create dirs for each season + region\n",
    "        directory_path = f\"data/ladder/season_{season}/{region}\"\n",
    "        \n",
    "        # if dir exists, delete\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "        \n",
    "        # create dirs\n",
    "        os.makedirs(directory_path)\n",
    "        \n",
    "        for rank, rows in ranks.items():\n",
    "            # convert every row (each list of dictionaries) into df\n",
    "            df = pd.DataFrame(rows)\n",
    "            # create csv based on season, region, rank\n",
    "            filename = f\"{directory_path}/{rank}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "'''\n",
    "31 seasons\n",
    "4 regions\n",
    "7 ranks\n",
    "\n",
    "==> ~868 CSVs\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5300",
   "language": "python",
   "name": "dsan5300"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
