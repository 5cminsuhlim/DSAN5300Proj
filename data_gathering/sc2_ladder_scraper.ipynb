{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\python\\311\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy in d:\\python\\311\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\python\\311\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: selenium in d:\\python\\311\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: webdriver-manager in d:\\python\\311\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: lxml in d:\\python\\311\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\python\\311\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\python\\311\\lib\\site-packages (from selenium) (1.26.13)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\python\\311\\lib\\site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\python\\311\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\python\\311\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in d:\\python\\311\\lib\\site-packages (from selenium) (4.10.0)\n",
      "Requirement already satisfied: requests in d:\\python\\311\\lib\\site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\python\\311\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in d:\\python\\311\\lib\\site-packages (from webdriver-manager) (24.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\python\\311\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\python\\311\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\python\\311\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in d:\\python\\311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\python\\311\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\python\\311\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\python\\311\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in d:\\python\\311\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\python\\311\\lib\\site-packages (from requests->webdriver-manager) (2.0.12)\n",
      "Requirement already satisfied: pycparser in d:\\python\\311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\python\\311\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir pandas numpy beautifulsoup4 selenium webdriver-manager lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import lxml\n",
    "import lxml.etree\n",
    "import time\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(seasons, id_anchor):\n",
    "    data_list = []\n",
    "\n",
    "    # setup selenium web driver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    for season in seasons:\n",
    "        max_rating = 99999 # arbitrarily high max rating for new season\n",
    "        page_idx = 0 # start from first page for new season\n",
    "\n",
    "        while True:\n",
    "            attempts = 0 # attempts counter\n",
    "            while attempts < 5:\n",
    "                print(f\"Season: {season}, Fetching page: {page_idx + 1}, Attempt: {attempts + 1}\")\n",
    "                url = f\"https://sc2pulse.nephest.com/sc2/?season={season}&queue=LOTV_1V1&team-type=ARRANGED&us=true&eu=true&kr=true&cn=true&bro=true&sil=true&gol=true&pla=true&dia=true&mas=true&gra=true&page={page_idx}&type=ladder&ratingAnchor={max_rating}&idAnchor={id_anchor}&count=1#ladder-top\"\n",
    "\n",
    "                driver.get(url)\n",
    "                delay = np.random.uniform(30,120)\n",
    "                time.sleep(delay) # adjust based on website tolerance\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                ladder_table_container = soup.find('div', id='ladder-table-container')\n",
    "                if ladder_table_container:\n",
    "                    tbody = ladder_table_container.find('tbody')\n",
    "                    rows = tbody.find_all('tr')\n",
    "                    if rows:\n",
    "                        for row in rows:\n",
    "                            player_data = {}\n",
    "                            player_data['Season'] = season\n",
    "                            \n",
    "                            mmr = int(row.find('td', class_='rating').text.strip())\n",
    "                            player_data['Rating'] = mmr\n",
    "                            \n",
    "                            if mmr >= 4800:\n",
    "                                rank = \"Grandmaster\"\n",
    "                            elif mmr >= 4250 and mmr < 4800:\n",
    "                                rank = \"Master\"\n",
    "                            elif mmr >= 3120 and mmr < 4250:\n",
    "                                rank = \"Diamond\"\n",
    "                            elif mmr >= 2680 and mmr < 3120:\n",
    "                                rank = \"Platinum\"\n",
    "                            elif mmr >= 2280 and mmr < 2680:\n",
    "                                rank = \"Gold\"\n",
    "                            elif mmr >= 1720 and mmr < 2280:\n",
    "                                rank = \"Silver\"\n",
    "                            else:\n",
    "                                rank = \"Bronze\"\n",
    "                            player_data['Rank'] = rank\n",
    "                            \n",
    "                            race_img = row.find('span', class_='race-percentage-entry').find('img', alt=True)\n",
    "                            player_data['Race'] = race_img['alt'].title() if race_img else 'Random'\n",
    "                            \n",
    "                            region_img = row.find('img', class_='table-image-long')\n",
    "                            player_data['Region'] = region_img['alt'].upper() if region_img else 'Unknown'\n",
    "                            \n",
    "                            data_list.append(player_data)\n",
    "\n",
    "                        break # break out of attempts loop upon success\n",
    "                    else:\n",
    "                        print(f\"Attempt {attempts + 1}: No data found on page {page_idx} for season {season}.\")\n",
    "                        attempts += 1\n",
    "                else:\n",
    "                    attempts += 1\n",
    "\n",
    "            if attempts == 5:\n",
    "                print(f\"After 5 attempts, no data could be retrieved for season {season} page {page_idx + 1}. Moving on...\")\n",
    "            page_idx += 1\n",
    "\n",
    "    driver.quit()\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season: 28, Fetching page: 1, Attempt: 1\n",
      "Season: 29, Fetching page: 1, Attempt: 1\n",
      "Season: 28, Fetching page: 2, Attempt: 1\n",
      "Season: 29, Fetching page: 2, Attempt: 1\n",
      "Season: 28, Fetching page: 3, Attempt: 1\n",
      "Season: 29, Fetching page: 3, Attempt: 1\n"
     ]
    }
   ],
   "source": [
    "total_seasons = np.arange(28, 59) # seasons 28 through 58\n",
    "season_splits = np.array_split(total_seasons, 2) # split seasons into 2 parts\n",
    "id_anchors = np.arange(len(season_splits))\n",
    "\n",
    "# parallelize web scraping\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:    \n",
    "    futures = [executor.submit(scrape, seasons, id_anchor) for seasons, id_anchor in zip(season_splits, id_anchors)]\n",
    "    \n",
    "    all_data = []\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        all_data.extend(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n31 seasons\\n4 regions\\n7 ranks\\n\\n==> 868 CSVs\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# organize data by season, region, rank\n",
    "organized_data = {}\n",
    "\n",
    "for row in all_data:\n",
    "    season = row['Season']\n",
    "    region = row['Region']\n",
    "    rank = row['Rank']\n",
    "    \n",
    "    if season not in organized_data:\n",
    "        organized_data[season] = {}\n",
    "    \n",
    "    if region not in organized_data[season]:\n",
    "        organized_data[season][region] = {}\n",
    "        \n",
    "    if rank not in organized_data[season][region]:\n",
    "        organized_data[season][region][rank] = []\n",
    "    \n",
    "    organized_data[season][region][rank].append(row)\n",
    "\n",
    "for season, regions in organized_data.items():\n",
    "    for region, ranks in regions.items():\n",
    "        # create dirs for each season + region\n",
    "        directory_path = f\"data/ladder/season_{season}/{region}\"\n",
    "        \n",
    "        # if dir exists, delete\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "        \n",
    "        # create dirs\n",
    "        os.makedirs(directory_path)\n",
    "        \n",
    "        for rank, rows in ranks.items():\n",
    "            # convert every row (each list of dictionaries) into df\n",
    "            df = pd.DataFrame(rows)\n",
    "            # create csv based on season, region, rank\n",
    "            filename = f\"{directory_path}/{rank}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "'''\n",
    "31 seasons\n",
    "4 regions\n",
    "7 ranks\n",
    "\n",
    "==> 868 CSVs\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5300",
   "language": "python",
   "name": "dsan5300"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
